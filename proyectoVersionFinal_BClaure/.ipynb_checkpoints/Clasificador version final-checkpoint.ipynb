{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Habilitar intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "49995  Seeing as the vote average was pretty low, and...   pos\n",
       "49996  The plot had some wretched, unbelievable twist...   pos\n",
       "49997  I am amazed at how this movie(and most others ...   pos\n",
       "49998  A Christmas Together actually came before my t...   pos\n",
       "49999  Working-class romantic drama from director Mar...   pos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('imdb_dataset.csv',encoding=\"ISO-8859-1\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prueba=data.sample(frac=0.2)\n",
    "palabrasParada = set( nltk.corpus.stopwords.words('english') + list(string.punctuation)+[\"...\",\"..\",\"hr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinhtml(conHTML):\n",
    "    limpiar_html = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    sin_html = re.sub(limpiar_html, '', conHTML)\n",
    "    return sin_html\n",
    "\n",
    "def quitar_html(palabras):\n",
    "    array = []\n",
    "    for palabra in  palabras:\n",
    "        sinHtml = sinhtml(palabra)\n",
    "        array.append(sinHtml)\n",
    "    return array\n",
    "\n",
    "def quitarComillas(palabras):\n",
    "    array = []\n",
    "    for palabra in  palabras:\n",
    "        palabra = palabra.strip(\"'\")\n",
    "        palabra = palabra.strip(\"`\")\n",
    "        array.append(palabra)\n",
    "    return array\n",
    "\n",
    "def Tokenizar(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras = nltk.word_tokenize(texto)#separa las palabras\n",
    "    return [palabra for palabra in palabras if palabra not in palabrasParada]#quita stopwords\n",
    "def listToString(lista):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(lista)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breaks heart movie appreciated underrated people forgot movies really nowadays think bum bum movies quite fun watching popcorn friends like transformers movies oriented hyper mega high budget like 300mln even higher special effects dumb movies without storyline kind movie despite course fun watching greatly made cgis gain anything essential kind movies. br br honestly think performance excellent especially busy philipps alongside erika christensen victor garber respect made movie oscar worth emotional performance busy philipps astonishing shame wont see oscar hands deserves'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listaDeComentarios=[]\n",
    "for index, row in data_prueba.iterrows():\n",
    "    tokenizado=Tokenizar(row['Review'])\n",
    "    sinHTML=quitar_html(tokenizado)\n",
    "    sinComillas=quitarComillas(sinHTML)\n",
    "    lista=listToString(sinComillas)  \n",
    "    listaDeComentarios.append(lista)\n",
    "listaDeComentarios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama침o del lista:  69044\n"
     ]
    }
   ],
   "source": [
    "def get_problem_vocabulary(normalized_corpus):\n",
    "    all_tokens = [] \n",
    "    for document in normalized_corpus:\n",
    "        all_tokens.extend(document.split())  \n",
    "    #[all_tokens.extend(document.split()) for document in normalized_corpus]\n",
    "    all_tokens_sorted = sorted(set(all_tokens))\n",
    "    \n",
    "    token_and_position = {}\n",
    "    for i, token in enumerate(all_tokens_sorted):\n",
    "        token_and_position[token] = i\n",
    "    \n",
    "    return token_and_position\n",
    "problem_vocabulary = get_problem_vocabulary(listaDeComentarios)\n",
    "print(\"Tama침o del lista: \",len(problem_vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(document, problem_vocabulary):\n",
    "    vector = np.zeros(len(problem_vocabulary),dtype=int)\n",
    "    for token in document.split():\n",
    "        vector[problem_vocabulary[token]] = 1\n",
    "    return vector\n",
    "def one_hot_matriz(lista):\n",
    "    matriz=[]\n",
    "    for i in lista:\n",
    "        matriz.append(one_hot_vector(i, problem_vocabulary))\n",
    "    return matriz\n",
    "        \n",
    "matriz=one_hot_matriz(listaDeComentarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de comentarios utilizados:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de comentarios utilizados: \" ,len(data_prueba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizDic=[]\n",
    "i=0\n",
    "for index, row in data_prueba.iterrows():\n",
    "    z = {'tokenizado':matriz[i],'clasificacion':row['Label']}\n",
    "    i=i+1\n",
    "    matrizDic.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaParaEntrenar=[]\n",
    "for i in matrizDic:\n",
    "    if(i[\"clasificacion\"]==\"pos\"):\n",
    "        listaParaEntrenar.append([i[\"tokenizado\"], 1])\n",
    "    else:\n",
    "        listaParaEntrenar.append([i[\"tokenizado\"], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listaParaEntrenar)\n",
    "dfExpandido = df[0].apply(pd.Series)\n",
    "dfExpandido[\"res\"]=df[1]\n",
    "tamMat=len(problem_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfExpandido.iloc[:, np.r_[0:tamMat]]\n",
    "y = dfExpandido[\"res\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 68374) (3000, 68374) (7000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training, 30% test\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeV4=RandomForestClassifier(n_estimators=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=88,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeV4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy del clasificador - version 4 : {0:.2f}'.format(accuracy_score(y_test, treeV4.predict(X_test))))\n",
    "# confusion matrix\n",
    "print('matriz de confusi칩n del clasificador - version 4: \\n {0}'.format(confusion_matrix(y_test, treeV4.predict(X_test))))\n",
    "# precision \n",
    "print('precision del clasificador - version 4 : {0:.2f}'.format(precision_score(y_test, treeV4.predict(X_test))))\n",
    "# recall\n",
    "print('recall del clasificador - version 4 : {0:.2f}'.format(recall_score(y_test, treeV4.predict(X_test))))\n",
    "# f1\n",
    "print('f1 del clasificador - version 4 : {0:.2f}'.format(f1_score(y_test, treeV4.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cree la carpeta 'clasificador' en el folder donde est치 el notebook\n",
    "ruta_archivo_clasificador = os.path.join(\"Clasificador\", \"tree_v4.pkl\")\n",
    "#Abrir el archivo para escribir contenido binario\n",
    "archivo_clasificador = open(ruta_archivo_clasificador, \"wb\")\n",
    "pickle.dump(treeV4, archivo_clasificador)\n",
    "archivo_clasificador.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_vocabulario = os.path.join(\"Clasificador\",\"VocabularioProblema.pkl\")\n",
    "\n",
    "#Abrir el archivo para escribir contenido binario\n",
    "archivo_vocabulario = open(ruta_archivo_vocabulario, \"wb\")\n",
    "\n",
    "pickle.dump(problem_vocabulary, archivo_vocabulario,protocol=2)\n",
    "\n",
    "archivo_vocabulario.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
