{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Habilitar intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Label\n",
       "0  Once again Mr. Costner has dragged out a movie...   neg\n",
       "1  This is an example of why the majority of acti...   neg\n",
       "2  First of all I hate those moronic rappers, who...   neg\n",
       "3  Not even the Beatles could write songs everyon...   neg\n",
       "4  Brass pictures (movies is not a fitting word f...   neg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('imdb_dataset.csv',encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  50000 non-null  object\n",
      " 1   Label   50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# print(data.shape) # (50000, 1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prueba=data.sample(frac=0.2)\n",
    "palabrasParada = set( nltk.corpus.stopwords.words('english') + list(string.punctuation)+[\"...\",\"..\",\"hr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24491</th>\n",
       "      <td>I avoided this film as a boy because I thought...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33530</th>\n",
       "      <td>This makes the third Errol Morris movie I've s...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38010</th>\n",
       "      <td>Lillian Hellman, one of America's most famous ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35321</th>\n",
       "      <td>I have read the novel Reaper of Ben Mezrich a ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36407</th>\n",
       "      <td>I caught this movie on Sci-Fi before heading i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19629</th>\n",
       "      <td>My Wife and I saw this movie once in 1989 and ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21154</th>\n",
       "      <td>In the early '80s, I recorded Honky Tonk Freew...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>I am almost tempted to demand my money back fr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>Holy crap, the beginning picked up where the f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30111</th>\n",
       "      <td>The script for this movie was probably found i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "24491  I avoided this film as a boy because I thought...   pos\n",
       "33530  This makes the third Errol Morris movie I've s...   neg\n",
       "38010  Lillian Hellman, one of America's most famous ...   pos\n",
       "35321  I have read the novel Reaper of Ben Mezrich a ...   neg\n",
       "36407  I caught this movie on Sci-Fi before heading i...   neg\n",
       "...                                                  ...   ...\n",
       "19629  My Wife and I saw this movie once in 1989 and ...   pos\n",
       "21154  In the early '80s, I recorded Honky Tonk Freew...   pos\n",
       "5669   I am almost tempted to demand my money back fr...   neg\n",
       "6646   Holy crap, the beginning picked up where the f...   neg\n",
       "30111  The script for this movie was probably found i...   neg\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardando data -> 20% = 10.000\n",
    "# # #Crear DataFrame de 10.000 \n",
    "# df = pd.DataFrame(poblacion) \n",
    "# muestra = df.sample(n = 10000)\n",
    "# muestra.head() #Es aleatorio, cada vez hay nuevos valores -> Resultados deberian ser parecidos \n",
    "\n",
    "#Exportar DataFrame\n",
    "data_prueba.to_csv ('imdb_dataset_prueba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinhtml(conHTML):\n",
    "    limpiar_html = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    sin_html = re.sub(limpiar_html, '', conHTML)\n",
    "    return sin_html\n",
    "\n",
    "def quitar_html(palabras):\n",
    "    array = []\n",
    "    for palabra in  palabras:\n",
    "        sinHtml = sinhtml(palabra)\n",
    "        array.append(sinHtml)\n",
    "    return array\n",
    "\n",
    "def quitarComillas(palabras):\n",
    "    array = []\n",
    "    for palabra in  palabras:\n",
    "        palabra = palabra.strip(\"'\")\n",
    "        palabra = palabra.strip(\"`\")\n",
    "        array.append(palabra)\n",
    "    return array\n",
    "\n",
    "def Tokenizar(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras = nltk.word_tokenize(texto)#separa las palabras\n",
    "    return [palabra for palabra in palabras if palabra not in palabrasParada]#quita stopwords\n",
    "def listToString(lista):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(lista)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'various nudity scenes reviewers referred poorly done body double obviously used ms. pacula reluctant scenes perhaps turned role offer. br br otherwise movie worse typical canadian movies reviewers pointed canadian movies generally poorly written lack entertainment value movies watchers hoping get perhaps canadian movie producers consciously trying  de-commercialize  movies forgotten important thing movies definition commercial thing ....'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listaDeComentarios=[]\n",
    "for index, row in data_prueba.iterrows():\n",
    "    tokenizado=Tokenizar(row['Review'])\n",
    "    sinHTML=quitar_html(tokenizado)\n",
    "    sinComillas=quitarComillas(sinHTML)\n",
    "    lista=listToString(sinComillas)  \n",
    "    listaDeComentarios.append(lista)\n",
    "listaDeComentarios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del lista:  69440\n"
     ]
    }
   ],
   "source": [
    "def get_problem_vocabulary(normalized_corpus):\n",
    "    all_tokens = [] \n",
    "    for document in normalized_corpus:\n",
    "        all_tokens.extend(document.split())  \n",
    "    all_tokens_sorted = sorted(set(all_tokens))\n",
    "    \n",
    "    token_and_position = {}\n",
    "    for i, token in enumerate(all_tokens_sorted):\n",
    "        token_and_position[token] = i\n",
    "    \n",
    "    return token_and_position\n",
    "problem_vocabulary = get_problem_vocabulary(listaDeComentarios)\n",
    "print(\"Tamaño del lista: \",len(problem_vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(document, problem_vocabulary):\n",
    "    vector = np.zeros(len(problem_vocabulary),dtype=int)\n",
    "    for token in document.split():\n",
    "        vector[problem_vocabulary[token]] = 1\n",
    "    return vector\n",
    "def one_hot_matriz(lista):\n",
    "    matriz=[]\n",
    "    for i in lista:\n",
    "        matriz.append(one_hot_vector(i, problem_vocabulary))\n",
    "    return matriz\n",
    "        \n",
    "matriz=one_hot_matriz(listaDeComentarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de comentarios utilizados:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de comentarios utilizados: \" ,len(data_prueba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizDic=[]\n",
    "i=0\n",
    "for index, row in data_prueba.iterrows():\n",
    "    z = {'tokenizado':matriz[i],'clasificacion':row['Label']}\n",
    "    i=i+1\n",
    "    matrizDic.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "listaParaEntrenar=[]\n",
    "for i in matrizDic:\n",
    "    if(i[\"clasificacion\"]==\"pos\"):\n",
    "        listaParaEntrenar.append([i[\"tokenizado\"], 1])\n",
    "    else:\n",
    "        listaParaEntrenar.append([i[\"tokenizado\"], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listaParaEntrenar)\n",
    "dfExpandido = df[0].apply(pd.Series)\n",
    "dfExpandido[\"res\"]=df[1]\n",
    "tamMat=len(problem_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfExpandido.iloc[:, np.r_[0:tamMat]]\n",
    "y = dfExpandido[\"res\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 69440) (3000, 69440) (7000,) (3000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training, 30% test\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeV4=RandomForestClassifier(n_estimators=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=88,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeV4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy del clasificador - version 4 : 0.83\n",
      "matriz de confusión del clasificador - version 4: \n",
      " [[1296  214]\n",
      " [ 283 1207]]\n",
      "precision del clasificador - version 4 : 0.85\n",
      "recall del clasificador - version 4 : 0.81\n",
      "f1 del clasificador - version 4 : 0.83\n"
     ]
    }
   ],
   "source": [
    "print('accuracy del clasificador - version 4 : {0:.2f}'.format(accuracy_score(y_test, treeV4.predict(X_test))))\n",
    "# confusion matrix\n",
    "print('matriz de confusión del clasificador - version 4: \\n {0}'.format(confusion_matrix(y_test, treeV4.predict(X_test))))\n",
    "# precision \n",
    "print('precision del clasificador - version 4 : {0:.2f}'.format(precision_score(y_test, treeV4.predict(X_test))))\n",
    "# recall\n",
    "print('recall del clasificador - version 4 : {0:.2f}'.format(recall_score(y_test, treeV4.predict(X_test))))\n",
    "# f1\n",
    "print('f1 del clasificador - version 4 : {0:.2f}'.format(f1_score(y_test, treeV4.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_clasificador = os.path.join(\"Clasificador\", \"tree_v4.pkl\")\n",
    "archivo_clasificador = open(ruta_archivo_clasificador, \"wb\")\n",
    "pickle.dump(treeV4, archivo_clasificador)\n",
    "archivo_clasificador.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_vocabulario = os.path.join(\"Clasificador\",\"VocabularioProblema.pkl\")\n",
    "\n",
    "#Abrir el archivo para escribir contenido binario\n",
    "archivo_vocabulario = open(ruta_archivo_vocabulario, \"wb\")\n",
    "\n",
    "pickle.dump(problem_vocabulary, archivo_vocabulario,protocol=2)\n",
    "\n",
    "archivo_vocabulario.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
